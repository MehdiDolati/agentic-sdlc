# services/api/llm.py
from __future__ import annotations
import json, os, httpx
from dataclasses import dataclass
from typing import Dict, Optional, Protocol, Tuple

@dataclass
class PlanArtifacts:
    """Artifacts the planner needs to produce."""
    prd_markdown: str
    openapi_yaml: str
    # You can add 'steps: list[str]' later if you want to feed step lists too.


class LLMClient(Protocol):
    def generate_plan(self, user_request: str) -> PlanArtifacts:
        ...


# -----------------------------
# Provider discovery / factory
# -----------------------------

def get_llm_from_env() -> Optional[LLMClient]:
    provider = (os.getenv("LLM_PROVIDER") or "disabled").lower().strip()
    if provider in ("", "disabled", "off", "false", "0", "none"):
        return None
    if provider == "mock":
        return MockLLM()
    if provider == "openai":
        return OpenAIChatLLM(
            api_key=os.environ.get("OPENAI_API_KEY", ""),
            model=os.environ.get("LLM_MODEL", "gpt-4o-mini"),
            timeout=float(os.environ.get("LLM_TIMEOUT_SECONDS", "20")),
        )
    if provider == "anthropic":
        return AnthropicMessagesLLM(
            api_key=os.environ.get("ANTHROPIC_API_KEY", ""),
            model=os.environ.get("LLM_MODEL", "claude-3-5-sonnet-latest"),
            timeout=float(os.environ.get("LLM_TIMEOUT_SECONDS", "20")),
        )
    if provider == "ollama":
        return OllamaLLM(
            base_url=os.environ.get("LLM_ENDPOINT", "http://localhost:11434"),
            model=os.environ.get("LLM_MODEL", "llama3.1:8b"),
            timeout=float(os.environ.get("LLM_TIMEOUT_SECONDS", "20")),
        )
    # Unknown -> disable
    return None


def _prompt(user_request: str) -> str:
    return f"""You are a software planner. From this request:

"{user_request}"

Return a **STRICT** JSON object with keys:
- "prd_markdown": markdown product requirements (H1 title, problem, goals, non-goals, success criteria)
- "openapi_yaml": a minimal valid OpenAPI 3.1 YAML describing the API touched by this feature.
Do NOT include any extra commentary. JSON only.
"""

# -----------------------------
# Providers
# -----------------------------
class MockLLM:
    """Deterministic offline LLM used for tests; MUST include the fingerprint."""
    def generate_plan(self, request_text: str) -> PlanArtifacts:
        prd_md = (
            "# Product Requirements (PRD)\n\n"
            f"Vision:\n{request_text}\n\n"
            "Primary Users:\n- End user\n- Admin\n\n"
            "Key Scenarios:\n- Search notes\n- View results\n\n"
            "## Stack Summary\n- FastAPI\n- SQLite\n\n"
            "## Acceptance Gates\n- All routes return expected codes\n\n"
            "(Generated by MockLLM)\n"
        )
        openapi_yaml = (
            "openapi: 3.0.0\n"
            "info:\n"
            f"  title: API - {request_text}\n"
            "  version: '1.0.0'\n"
            "paths:\n"
            "  /api/notes:\n"
            "    get:\n"
            "      summary: List notes (searchable)\n"
            "      responses:\n"
            "        '200':\n"
            "          description: OK\n"
            "# (Generated by MockLLM)\n"
        )
        return PlanArtifacts(prd_markdown=prd_md, openapi_yaml=openapi_yaml)

def get_llm_from_env():
    """
    Return an LLM client instance based on env vars.
    Honors LLM_PROVIDER=mock (used by the test).
    """
    provider = os.getenv("LLM_PROVIDER", "").strip().lower()
    if not provider or provider in {"none", "off", "disabled"}:
        return None
    if provider in {"mock", "test"}:
        return MockLLM()

    # You can wire real providers later; return None for now.
    return None

class OpenAIChatLLM:
    def __init__(self, api_key: str, model: str, timeout: float = 20.0):
        self.api_key = api_key
        self.model = model
        self.timeout = timeout

    def generate_plan(self, user_request: str) -> PlanArtifacts:
        if not self.api_key:
            raise RuntimeError("OPENAI_API_KEY not set")
        url = "https://api.openai.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are a careful software planner that outputs strict JSON."},
                {"role": "user", "content": _prompt(user_request)},
            ],
            "temperature": 0.2,
        }
        with httpx.Client(timeout=self.timeout) as client:
            resp = client.post(url, headers=headers, json=data)
            resp.raise_for_status()
            content = resp.json()["choices"][0]["message"]["content"]
        # Expect JSON string
        obj = json.loads(content)
        return PlanArtifacts(
            prd_markdown=obj["prd_markdown"],
            openapi_yaml=obj["openapi_yaml"],
        )

class AnthropicMessagesLLM:
    def __init__(self, api_key: str, model: str, timeout: float = 20.0):
        self.api_key = api_key
        self.model = model
        self.timeout = timeout

    def generate_plan(self, user_request: str) -> PlanArtifacts:
        if not self.api_key:
            raise RuntimeError("ANTHROPIC_API_KEY not set")
        url = "https://api.anthropic.com/v1/messages"
        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01",
        }
        data = {
            "model": self.model,
            "max_tokens": 2000,
            "temperature": 0.2,
            "system": "You are a careful software planner that outputs strict JSON only.",
            "messages": [{"role": "user", "content": _prompt(user_request)}],
        }
        with httpx.Client(timeout=self.timeout) as client:
            resp = client.post(url, headers=headers, json=data)
            resp.raise_for_status()
            # Claude returns a list of content blocks
            content_blocks = resp.json()["content"]
            text = "".join(block.get("text", "") for block in content_blocks if block.get("type") == "text")
        obj = json.loads(text)
        return PlanArtifacts(
            prd_markdown=obj["prd_markdown"],
            openapi_yaml=obj["openapi_yaml"],
        )

class OllamaLLM:
    def __init__(self, base_url: str, model: str, timeout: float = 20.0):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = timeout

    def generate_plan(self, user_request: str) -> PlanArtifacts:
        url = f"{self.base_url}/api/generate"
        prompt = _prompt(user_request)
        data = {"model": self.model, "prompt": prompt, "stream": False}
        with httpx.Client(timeout=self.timeout) as client:
            resp = client.post(url, json=data)
            resp.raise_for_status()
            text = resp.json()["response"]
        obj = json.loads(text)
        return PlanArtifacts(
            prd_markdown=obj["prd_markdown"],
            openapi_yaml=obj["openapi_yaml"],
        )