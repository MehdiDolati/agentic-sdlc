# services/api/llm.py
from __future__ import annotations
import json, os, httpx
from dataclasses import dataclass
from typing import Dict, List, Optional, Protocol

@dataclass
class PlanArtifacts:
    """Artifacts the planner needs to produce."""
    prd_markdown: str
    openapi_yaml: str
    implementation_plan: List[Dict[str, object]]


class LLMClient(Protocol):
    def generate_plan(self, user_request: str) -> PlanArtifacts:
        ...


def _prompt(user_request: str) -> str:
    return f"""You are a senior software planner. From this request:

"{user_request}"

Return a **STRICT** JSON object with keys:
- "prd_markdown": markdown product requirements (H1 title, problem, goals, non-goals, success criteria)
- "openapi_yaml": a minimal valid OpenAPI 3.1 YAML describing the API touched by this feature
- "implementation_plan": an array where each item describes an implementation plan with the following shape:
  - "id": short stable identifier (e.g. "plan-1")
  - "name": short title for the plan
  - "description": summary of the plan
  - "priority": one of ["critical", "high", "medium", "low"]
  - "size_estimate_days": positive integer duration estimate in days
  - "features": array of feature objects. Each feature object must contain:
      * "id": identifier (e.g. "feature-1")
      * "name": short actionable title
      * "description": 1-2 sentence description of the work
      * "priority": one of ["critical", "high", "medium", "low"]
      * "size_estimate_hours": positive integer effort estimate in hours
      * "acceptance_criteria": array of bullet strings describing completion requirements
Do NOT include any additional commentary. JSON only.
"""

# -----------------------------
# Providers
# -----------------------------
class MockLLM:
    """Deterministic offline LLM used for tests; MUST include the fingerprint."""
    def generate_plan(self, request_text: str) -> PlanArtifacts:
        prd_md = (
            "# Product Requirements (PRD)\n\n"
            f"Vision:\n{request_text}\n\n"
            "Primary Users:\n- End user\n- Admin\n\n"
            "Key Scenarios:\n- Search notes\n- View results\n\n"
            "## Stack Summary\n- FastAPI\n- SQLite\n\n"
            "## Acceptance Gates\n- All routes return expected codes\n\n"
            "(Generated by MockLLM)\n"
        )
        openapi_yaml = (
            "openapi: 3.0.0\n"
            "info:\n"
            f"  title: API - {request_text}\n"
            "  version: '1.0.0'\n"
            "paths:\n"
            "  /api/notes:\n"
            "    get:\n"
            "      summary: List notes (searchable)\n"
            "      responses:\n"
            "        '200':\n"
            "          description: OK\n"
            "# (Generated by MockLLM)\n"
        )
        implementation_plan = [
            {
                "id": "plan-1",
                "name": "Primary Delivery Plan",
                "description": "Outline the core workstreams required to satisfy the user request with backend, frontend, and testing coverage.",
                "priority": "high",
                "size_estimate_days": 12,
                "features": [
                    {
                        "id": "feature-1",
                        "name": "Clarify requirements and scope",
                        "description": "Workshop detailed requirements, edge cases, and success criteria with stakeholders.",
                        "priority": "high",
                        "size_estimate_hours": 10,
                        "acceptance_criteria": [
                            "Requirements document approved",
                            "Edge cases are documented",
                            "Dependencies and risks are identified"
                        ],
                    },
                    {
                        "id": "feature-2",
                        "name": "Deliver service implementation",
                        "description": "Implement backend logic, persistence layer, and APIs aligned with the PRD.",
                        "priority": "critical",
                        "size_estimate_hours": 24,
                        "acceptance_criteria": [
                            "Endpoints pass contract and integration tests",
                            "Data model changes are migrated and documented",
                            "Monitoring hooks are in place"
                        ],
                    },
                    {
                        "id": "feature-3",
                        "name": "Validate user experience",
                        "description": "Build or update user flows and ensure usability with QA sign-off.",
                        "priority": "medium",
                        "size_estimate_hours": 18,
                        "acceptance_criteria": [
                            "Front-end matches design specs",
                            "Accessibility and browser regression checks complete",
                            "Stakeholder demo feedback addressed"
                        ],
                    },
                ],
            }
        ]
        return PlanArtifacts(
            prd_markdown=prd_md,
            openapi_yaml=openapi_yaml,
            implementation_plan=implementation_plan,
        )

def get_llm_from_env() -> Optional[LLMClient]:
    """
    Return an LLM client instance based on env vars.
    Honors LLM_PROVIDER=mock (used by the test).
    """
    provider = os.getenv("LLM_PROVIDER", "").strip().lower()
    if not provider or provider in {"none", "off", "disabled"}:
        return None
    if provider in {"mock", "test"}:
        return MockLLM()
    if provider == "supabase":
        return SupabaseLLM(
            supabase_url=os.environ.get("SUPABASE_URL", ""),
            supabase_key=os.environ.get("SUPABASE_ANON_KEY", ""),
            timeout=float(os.environ.get("LLM_TIMEOUT_SECONDS", "20")),
        )
    if provider == "openai":
        return OpenAIChatLLM(
            api_key=os.environ.get("OPENAI_API_KEY", ""),
            model=os.environ.get("LLM_MODEL", "gpt-4o-mini"),
            timeout=float(os.environ.get("LLM_TIMEOUT_SECONDS", "20")),
        )
    if provider == "anthropic":
        return AnthropicMessagesLLM(
            api_key=os.environ.get("ANTHROPIC_API_KEY", ""),
            model=os.environ.get("LLM_MODEL", "claude-3-5-sonnet-latest"),
            timeout=float(os.environ.get("LLM_TIMEOUT_SECONDS", "20")),
        )
    if provider == "ollama":
        return OllamaLLM(
            base_url=os.environ.get("LLM_ENDPOINT", "http://localhost:11434"),
            model=os.environ.get("LLM_MODEL", "llama3.1:8b"),
            timeout=float(os.environ.get("LLM_TIMEOUT_SECONDS", "20")),
        )
    # Unknown -> disable
    return None

class OpenAIChatLLM:
    def __init__(self, api_key: str, model: str, timeout: float = 20.0):
        self.api_key = api_key
        self.model = model
        self.timeout = timeout

    def generate_plan(self, user_request: str) -> PlanArtifacts:
        if not self.api_key:
            raise RuntimeError("OPENAI_API_KEY not set")
        url = "https://api.openai.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are a careful software planner that outputs strict JSON."},
                {"role": "user", "content": _prompt(user_request)},
            ],
            "temperature": 0.2,
        }
        with httpx.Client(timeout=self.timeout) as client:
            resp = client.post(url, headers=headers, json=data)
            resp.raise_for_status()
            content = resp.json()["choices"][0]["message"]["content"]
        # Expect JSON string
        obj = json.loads(content)
        return PlanArtifacts(
            prd_markdown=obj["prd_markdown"],
            openapi_yaml=obj["openapi_yaml"],
            implementation_plan=obj.get("implementation_plan", []),
        )

class AnthropicMessagesLLM:
    def __init__(self, api_key: str, model: str, timeout: float = 20.0):
        self.api_key = api_key
        self.model = model
        self.timeout = timeout

    def generate_plan(self, user_request: str) -> PlanArtifacts:
        if not self.api_key:
            raise RuntimeError("ANTHROPIC_API_KEY not set")
        url = "https://api.anthropic.com/v1/messages"
        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01",
        }
        data = {
            "model": self.model,
            "max_tokens": 2000,
            "temperature": 0.2,
            "system": "You are a careful software planner that outputs strict JSON only.",
            "messages": [{"role": "user", "content": _prompt(user_request)}],
        }
        with httpx.Client(timeout=self.timeout) as client:
            resp = client.post(url, headers=headers, json=data)
            resp.raise_for_status()
            # Claude returns a list of content blocks
            content_blocks = resp.json()["content"]
            text = "".join(block.get("text", "") for block in content_blocks if block.get("type") == "text")
        obj = json.loads(text)
        return PlanArtifacts(
            prd_markdown=obj["prd_markdown"],
            openapi_yaml=obj["openapi_yaml"],
            implementation_plan=obj.get("implementation_plan", []),
        )

class OllamaLLM:
    def __init__(self, base_url: str, model: str, timeout: float = 20.0):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = timeout

    def generate_plan(self, user_request: str) -> PlanArtifacts:
        url = f"{self.base_url}/api/generate"
        prompt = _prompt(user_request)
        data = {"model": self.model, "prompt": prompt, "stream": False}
        with httpx.Client(timeout=self.timeout) as client:
            resp = client.post(url, json=data)
            resp.raise_for_status()
            text = resp.json()["response"]
        obj = json.loads(text)
        return PlanArtifacts(
            prd_markdown=obj["prd_markdown"],
            openapi_yaml=obj["openapi_yaml"],
            implementation_plan=obj.get("implementation_plan", []),
        )

class SupabaseLLM:
    def __init__(self, supabase_url: str, supabase_key: str, timeout: float = 20.0):
        self.supabase_url = supabase_url.rstrip("/")
        self.supabase_key = supabase_key
        self.timeout = timeout

    def generate_plan(self, user_request: str) -> PlanArtifacts:
        if not self.supabase_url or not self.supabase_key:
            raise RuntimeError("SUPABASE_URL and SUPABASE_ANON_KEY must be set")

        url = f"{self.supabase_url}/functions/v1/chat"
        headers = {
            "Authorization": f"Bearer {self.supabase_key}",
            "Content-Type": "application/json",
        }

        system_prompt = """You are a senior software planner. From the user's request you MUST return a strict JSON object with keys:
- "prd_markdown": markdown product requirements (H1 title, problem, goals, non-goals, success criteria)
- "openapi_yaml": a minimal valid OpenAPI 3.1 YAML describing the API touched by this feature
- "implementation_plan": an array of plans. Each plan must include: id, name, description, priority (critical/high/medium/low), size_estimate_days (integer), and a "features" array. Each feature must include: id, name, description, priority, size_estimate_hours (integer), acceptance_criteria (array of bullet strings).
Return JSON only with this structure. Do not wrap in markdown. Do not add commentary."""

        data = {
            "messages": [
                {"role": "user", "content": _prompt(user_request)}
            ],
            "systemPrompt": system_prompt
        }

        with httpx.Client(timeout=self.timeout) as client:
            resp = client.post(url, headers=headers, json=data)
            resp.raise_for_status()

            # Supabase returns streaming response, need to parse it
            content = ""
            for line in resp.iter_lines():
                line = line.decode('utf-8') if isinstance(line, bytes) else line
                if line.startswith("data: "):
                    data_str = line[6:]
                    if data_str and data_str != "[DONE]":
                        try:
                            parsed = json.loads(data_str)
                            chunk = parsed.get("choices", [{}])[0].get("delta", {}).get("content", "")
                            content += chunk
                        except json.JSONDecodeError:
                            continue

        # Parse the accumulated content as JSON
        if not content.strip():
            raise RuntimeError("Empty response from Supabase AI")

        # Strip markdown code blocks if present
        content = content.strip()
        if content.startswith("```json"):
            content = content[7:]  # Remove ```json
        if content.startswith("```"):
            content = content[3:]  # Remove ```
        if content.endswith("```"):
            content = content[:-3]  # Remove trailing ```
        content = content.strip()

        obj = json.loads(content)
        return PlanArtifacts(
            prd_markdown=obj["prd_markdown"],
            openapi_yaml=obj["openapi_yaml"],
            implementation_plan=obj.get("implementation_plan", []),
        )

    def generate_text(self, user_prompt: str, system_prompt: str = "") -> str:
        """Generate plain text response (not JSON) from Supabase LLM."""
        if not self.supabase_url or not self.supabase_key:
            raise RuntimeError("SUPABASE_URL and SUPABASE_ANON_KEY must be set")

        url = f"{self.supabase_url}/functions/v1/chat"
        headers = {
            "Authorization": f"Bearer {self.supabase_key}",
            "Content-Type": "application/json",
        }

        data = {
            "messages": [
                {"role": "user", "content": user_prompt}
            ],
            "systemPrompt": system_prompt or "You are a helpful AI assistant."
        }

        with httpx.Client(timeout=self.timeout) as client:
            resp = client.post(url, headers=headers, json=data)
            resp.raise_for_status()

            # Supabase returns streaming response
            content = ""
            for line in resp.iter_lines():
                line = line.decode('utf-8') if isinstance(line, bytes) else line
                if line.startswith("data: "):
                    data_str = line[6:]
                    if data_str and data_str != "[DONE]":
                        try:
                            parsed = json.loads(data_str)
                            chunk = parsed.get("choices", [{}])[0].get("delta", {}).get("content", "")
                            content += chunk
                        except json.JSONDecodeError:
                            continue

        if not content.strip():
            raise RuntimeError("Empty response from Supabase AI")

        return content.strip()